{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a482e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "import zipfile\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import xgboost as xgb\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fea41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/tsunami_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c5de29",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data = df.isnull().sum()\n",
    "missing_percentage = (missing_data / len(df)) * 100\n",
    "missing_info = pd.DataFrame({\n",
    "    'Missing Count': missing_data,\n",
    "    'Missing Percentage': missing_percentage\n",
    "}).sort_values('Missing Percentage', ascending=False)\n",
    "\n",
    "categorical_cols = ['EVENT_VALIDITY', 'CAUSE', 'REGION']\n",
    "for col in categorical_cols:\n",
    "    if col in df.columns:\n",
    "        df[col].value_counts().head(10)\n",
    "\n",
    "if 'TS_INTENSITY' in df.columns:\n",
    "    ts_intensity_counts = df['TS_INTENSITY'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ddb47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df.copy()\n",
    "df_clean = df_clean.dropna(how='all')\n",
    "df_clean = df_clean.dropna(subset=['EQ_MAGNITUDE'])\n",
    "df_clean = df_clean[(df_clean['EQ_MAGNITUDE'] >= 0) & (df_clean['EQ_MAGNITUDE'] <= 10)]\n",
    "\n",
    "if 'EQ_DEPTH' in df_clean.columns:\n",
    "    median_depth = df_clean['EQ_DEPTH'].median()\n",
    "    df_clean['EQ_DEPTH'] = df_clean['EQ_DEPTH'].fillna(median_depth)\n",
    "    df_clean = df_clean[(df_clean['EQ_DEPTH'] >= 0) & (df_clean['EQ_DEPTH'] <= 800)]\n",
    "\n",
    "if 'LATITUDE' in df_clean.columns and 'LONGITUDE' in df_clean.columns:\n",
    "    df_clean = df_clean[\n",
    "        (df_clean['LATITUDE'].between(-90, 90)) & \n",
    "        (df_clean['LONGITUDE'].between(-180, 180))\n",
    "    ]\n",
    "\n",
    "df_clean['TSUNAMI_OCCURRED'] = 0\n",
    "if 'TS_INTENSITY' in df_clean.columns:\n",
    "    df_clean.loc[df_clean['TS_INTENSITY'].notna() & (df_clean['TS_INTENSITY'] > 0), 'TSUNAMI_OCCURRED'] = 1\n",
    "\n",
    "if 'EVENT_VALIDITY' in df_clean.columns:\n",
    "    tsunami_indicators = ['Definite Tsunami', 'Probable Tsunami']\n",
    "    mask = df_clean['EVENT_VALIDITY'].isin(tsunami_indicators)\n",
    "    df_clean.loc[mask, 'TSUNAMI_OCCURRED'] = 1\n",
    "\n",
    "if 'YEAR' in df_clean.columns:\n",
    "    df_clean = df_clean[(df_clean['YEAR'] >= 1800) | df_clean['YEAR'].isna()]\n",
    "\n",
    "if 'CAUSE' in df_clean.columns:\n",
    "    df_clean['CAUSE'] = df_clean['CAUSE'].fillna('Unknown')\n",
    "\n",
    "if 'REGION' in df_clean.columns:\n",
    "    df_clean['REGION'] = df_clean['REGION'].fillna('Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6552e278",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = df_clean.copy()\n",
    "\n",
    "features_df['MAG_CATEGORY'] = pd.cut(\n",
    "    features_df['EQ_MAGNITUDE'], \n",
    "    bins=[0, 4.5, 6.0, 7.0, 8.0, 10.0],\n",
    "    labels=['Weak', 'Moderate', 'Strong', 'Major', 'Great']\n",
    ")\n",
    "\n",
    "features_df['MAG_SQUARED'] = features_df['EQ_MAGNITUDE'] ** 2\n",
    "features_df['IS_MAJOR_EQ'] = (features_df['EQ_MAGNITUDE'] >= 7.0).astype(int)\n",
    "\n",
    "if 'EQ_DEPTH' in features_df.columns:\n",
    "    features_df['DEPTH_CATEGORY'] = pd.cut(\n",
    "        features_df['EQ_DEPTH'],\n",
    "        bins=[0, 70, 300, 700, 800],\n",
    "        labels=['Shallow', 'Intermediate', 'Deep', 'Very_Deep']\n",
    "    )\n",
    "    \n",
    "    features_df['IS_SHALLOW'] = (features_df['EQ_DEPTH'] <= 70).astype(int)\n",
    "    features_df['DEPTH_MAG_RATIO'] = features_df['EQ_DEPTH'] / features_df['EQ_MAGNITUDE']\n",
    "\n",
    "def assign_risk_zone(lat, lon):\n",
    "    if pd.isna(lat) or pd.isna(lon):\n",
    "        return 'Unknown'\n",
    "    \n",
    "    if (lat >= 35 and lat <= 45 and lon >= 135 and lon <= 145) or \\\n",
    "       (lat >= -50 and lat <= -30 and lon >= -80 and lon <= -60) or \\\n",
    "       (lat >= 30 and lat <= 50 and lon >= -130 and lon <= -110):\n",
    "        return 'Very_High'\n",
    "    \n",
    "    elif (lon >= -180 and lon <= -100) or (lon >= 100 and lon <= 180):\n",
    "        if abs(lat) <= 60:\n",
    "            return 'High'\n",
    "    \n",
    "    elif (lon >= -100 and lon <= 20):\n",
    "        return 'Moderate'\n",
    "    \n",
    "    elif (lat >= 30 and lat <= 45 and lon >= 0 and lon <= 40):\n",
    "        return 'Low_Moderate'\n",
    "    \n",
    "    return 'Low'\n",
    "\n",
    "if 'LATITUDE' in features_df.columns and 'LONGITUDE' in features_df.columns:\n",
    "    features_df['RISK_ZONE'] = features_df.apply(\n",
    "        lambda row: assign_risk_zone(row['LATITUDE'], row['LONGITUDE']), axis=1\n",
    "    )\n",
    "\n",
    "def is_oceanic(lat, lon):\n",
    "    if pd.isna(lat) or pd.isna(lon):\n",
    "        return 0\n",
    "    \n",
    "    if lat >= 25 and lat <= 70 and lon >= -160 and lon <= -50:\n",
    "        return 0\n",
    "    elif lat >= 35 and lat <= 75 and lon >= -10 and lon <= 180:\n",
    "        return 0\n",
    "    elif lat >= -35 and lat <= 35 and lon >= -20 and lon <= 55:\n",
    "        return 0\n",
    "    elif lat >= -45 and lat <= -10 and lon >= 110 and lon <= 155:\n",
    "        return 0\n",
    "    elif lat >= -55 and lat <= 15 and lon >= -85 and lon <= -30:\n",
    "        return 0\n",
    "    \n",
    "    return 1\n",
    "\n",
    "if 'LATITUDE' in features_df.columns and 'LONGITUDE' in features_df.columns:\n",
    "    features_df['IS_OCEANIC'] = features_df.apply(\n",
    "        lambda row: is_oceanic(row['LATITUDE'], row['LONGITUDE']), axis=1\n",
    "    )\n",
    "\n",
    "if 'YEAR' in features_df.columns:\n",
    "    features_df['DECADE'] = (features_df['YEAR'] // 10) * 10\n",
    "    features_df['IS_RECENT'] = (features_df['YEAR'] >= 1950).astype(int)\n",
    "\n",
    "if 'MONTH' in features_df.columns:\n",
    "    features_df['SEASON'] = features_df['MONTH'].apply(\n",
    "        lambda x: 'Winter' if x in [12, 1, 2] else\n",
    "                  'Spring' if x in [3, 4, 5] else\n",
    "                  'Summer' if x in [6, 7, 8] else\n",
    "                  'Fall' if x in [9, 10, 11] else 'Unknown'\n",
    "    )\n",
    "\n",
    "label_encoders = {}\n",
    "categorical_features = ['RISK_ZONE', 'MAG_CATEGORY', 'DEPTH_CATEGORY', 'CAUSE', 'REGION']\n",
    "\n",
    "for feature in categorical_features:\n",
    "    if feature in features_df.columns:\n",
    "        le = LabelEncoder()\n",
    "        feature_series = features_df[feature].astype(str).fillna('Unknown')\n",
    "        if hasattr(feature_series, 'cat'):\n",
    "            feature_series = feature_series.cat.add_categories('Unknown').fillna('Unknown')\n",
    "        features_df[f'{feature}_ENCODED'] = le.fit_transform(feature_series)\n",
    "        label_encoders[feature] = le\n",
    "\n",
    "if 'TSUNAMI_OCCURRED' in features_df.columns:\n",
    "    numeric_features = features_df.select_dtypes(include=[np.number]).columns\n",
    "    correlations = features_df[numeric_features].corr()['TSUNAMI_OCCURRED'].sort_values(key=abs, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9053e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [\n",
    "    'EQ_MAGNITUDE', 'EQ_DEPTH', 'LATITUDE', 'LONGITUDE',\n",
    "    'MAG_SQUARED', 'IS_MAJOR_EQ', 'IS_SHALLOW', 'IS_OCEANIC',\n",
    "    'RISK_ZONE_ENCODED', 'MAG_CATEGORY_ENCODED', 'DEPTH_CATEGORY_ENCODED'\n",
    "]\n",
    "\n",
    "available_features = []\n",
    "for col in feature_columns:\n",
    "    if col in features_df.columns:\n",
    "        available_features.append(col)\n",
    "\n",
    "ml_data = features_df.dropna(subset=['TSUNAMI_OCCURRED']).copy()\n",
    "\n",
    "X = ml_data[available_features].copy()\n",
    "y = ml_data['TSUNAMI_OCCURRED'].copy()\n",
    "\n",
    "for col in X.columns:\n",
    "    if X[col].dtype in ['float64', 'int64']:\n",
    "        X[col] = X[col].fillna(X[col].median())\n",
    "    else:\n",
    "        X[col] = X[col].fillna(0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_minmax = minmax_scaler.fit_transform(X)\n",
    "\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
    "X_minmax_df = pd.DataFrame(X_minmax, columns=X.columns, index=X.index)\n",
    "\n",
    "feature_stats = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Mean': X.mean(),\n",
    "    'Std': X.std(),\n",
    "    'Min': X.min(),\n",
    "    'Max': X.max(),\n",
    "    'Missing_Count': X.isnull().sum()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d3d384",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled_df, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
    "    X_train, y_train,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_train\n",
    ")\n",
    "\n",
    "split_data = {\n",
    "    'X_train': X_train_split,\n",
    "    'X_val': X_val,\n",
    "    'X_test': X_test,\n",
    "    'y_train': y_train_split,\n",
    "    'y_val': y_val,\n",
    "    'y_test': y_test,\n",
    "    'feature_names': list(X.columns),\n",
    "    'scaler': scaler\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c132a747",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        random_state=42,\n",
    "        class_weight='balanced'\n",
    "    ),\n",
    "    'XGBoost': xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        scale_pos_weight=len(y_train_split[y_train_split==0])/len(y_train_split[y_train_split==1])\n",
    "    ),\n",
    "    'Support Vector Machine': SVC(\n",
    "        kernel='rbf',\n",
    "        C=1.0,\n",
    "        gamma='scale',\n",
    "        random_state=42,\n",
    "        class_weight='balanced',\n",
    "        probability=True\n",
    "    ),\n",
    "    'Logistic Regression': LogisticRegression(\n",
    "        C=1.0,\n",
    "        random_state=42,\n",
    "        class_weight='balanced',\n",
    "        max_iter=1000\n",
    "    ),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5,\n",
    "        random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "trained_models = {}\n",
    "model_scores = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train_split, y_train_split)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    y_pred = model.predict(X_val)\n",
    "    y_pred_proba = model.predict_proba(X_val)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    precision = precision_score(y_val, y_pred)\n",
    "    recall = recall_score(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred)\n",
    "    \n",
    "    cv_scores = cross_val_score(model, X_train_split, y_train_split, cv=5, scoring='f1')\n",
    "    \n",
    "    trained_models[name] = model\n",
    "    model_scores[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'training_time': training_time\n",
    "    }\n",
    "\n",
    "results_df = pd.DataFrame(model_scores).T\n",
    "results_df = results_df.round(3)\n",
    "\n",
    "best_model_name = results_df['f1'].idxmax()\n",
    "best_model = trained_models[best_model_name]\n",
    "\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X_train_split.columns,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebe947d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = best_model.predict(X_test)\n",
    "y_test_pred_proba = best_model.predict_proba(X_test)[:, 1] if hasattr(best_model, 'predict_proba') else None\n",
    "\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_precision = precision_score(y_test, y_test_pred)\n",
    "test_recall = recall_score(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "performance_summary = {\n",
    "    'Model': best_model_name,\n",
    "    'Test_Accuracy': test_accuracy,\n",
    "    'Test_Precision': test_precision,\n",
    "    'Test_Recall': test_recall,\n",
    "    'Test_F1': test_f1,\n",
    "    'True_Negatives': cm[0,0],\n",
    "    'False_Positives': cm[0,1],\n",
    "    'False_Negatives': cm[1,0],\n",
    "    'True_Positives': cm[1,1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694a32de",
   "metadata": {},
   "outputs": [],
   "source": [
    "USGS_ENDPOINTS = {\n",
    "    'past_hour_m45': 'https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/4.5_hour.geojson',\n",
    "    'past_day_m45': 'https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/4.5_day.geojson',\n",
    "    'past_hour_m25': 'https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/2.5_hour.geojson',\n",
    "    'past_day_all': 'https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/all_day.geojson'\n",
    "}\n",
    "\n",
    "def fetch_earthquake_data(endpoint_key='past_hour_m45', timeout=10):\n",
    "    try:\n",
    "        url = USGS_ENDPOINTS[endpoint_key]\n",
    "        response = requests.get(url, timeout=timeout)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        return data\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def parse_earthquake_features(earthquake_data):\n",
    "    if not earthquake_data or 'features' not in earthquake_data:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    earthquakes = []\n",
    "    \n",
    "    for feature in earthquake_data['features']:\n",
    "        props = feature['properties']\n",
    "        coords = feature['geometry']['coordinates']\n",
    "        \n",
    "        earthquake = {\n",
    "            'id': feature['id'],\n",
    "            'magnitude': props.get('mag'),\n",
    "            'place': props.get('place', ''),\n",
    "            'time': pd.to_datetime(props.get('time'), unit='ms'),\n",
    "            'longitude': coords[0] if len(coords) > 0 else None,\n",
    "            'latitude': coords[1] if len(coords) > 1 else None,\n",
    "            'depth': coords[2] if len(coords) > 2 else None,\n",
    "            'magType': props.get('magType', ''),\n",
    "            'nst': props.get('nst'),\n",
    "            'gap': props.get('gap'),\n",
    "            'dmin': props.get('dmin'),\n",
    "            'rms': props.get('rms'),\n",
    "            'net': props.get('net', ''),\n",
    "            'updated': pd.to_datetime(props.get('updated'), unit='ms') if props.get('updated') else None,\n",
    "            'type': props.get('type', ''),\n",
    "            'status': props.get('status', ''),\n",
    "            'tsunami': props.get('tsunami', 0),\n",
    "            'url': props.get('url', '')\n",
    "        }\n",
    "        earthquakes.append(earthquake)\n",
    "    \n",
    "    df = pd.DataFrame(earthquakes)\n",
    "    return df\n",
    "\n",
    "test_data = fetch_earthquake_data('past_hour_m45')\n",
    "\n",
    "if test_data:\n",
    "    test_df = parse_earthquake_features(test_data)\n",
    "\n",
    "def get_earthquake_monitoring_status():\n",
    "    status = {\n",
    "        'timestamp': datetime.now(),\n",
    "        'api_status': 'unknown',\n",
    "        'last_earthquake_count': 0,\n",
    "        'connection_test': False\n",
    "    }\n",
    "    \n",
    "    test_data = fetch_earthquake_data('past_hour_m25')\n",
    "    if test_data:\n",
    "        status['api_status'] = 'connected'\n",
    "        status['last_earthquake_count'] = len(test_data.get('features', []))\n",
    "        status['connection_test'] = True\n",
    "    else:\n",
    "        status['api_status'] = 'disconnected'\n",
    "    \n",
    "    return status\n",
    "\n",
    "monitoring_status = get_earthquake_monitoring_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67238656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_tsunami_threats(earthquake_df, min_magnitude=6.5, max_depth=70, oceanic_only=True):\n",
    "    if earthquake_df.empty:\n",
    "        return earthquake_df\n",
    "    \n",
    "    threats = earthquake_df.copy()\n",
    "    threats = threats[threats['magnitude'] >= min_magnitude]\n",
    "    threats = threats[threats['depth'] <= max_depth]\n",
    "    \n",
    "    if oceanic_only:\n",
    "        threats['is_oceanic'] = threats.apply(\n",
    "            lambda row: is_oceanic(row['latitude'], row['longitude']), axis=1\n",
    "        )\n",
    "        threats = threats[threats['is_oceanic'] == 1]\n",
    "    \n",
    "    return threats\n",
    "\n",
    "def prepare_features_for_prediction(earthquake_df):\n",
    "    if earthquake_df.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    features = earthquake_df.copy()\n",
    "    \n",
    "    feature_mapping = {\n",
    "        'magnitude': 'EQ_MAGNITUDE',\n",
    "        'depth': 'EQ_DEPTH',\n",
    "        'latitude': 'LATITUDE',\n",
    "        'longitude': 'LONGITUDE'\n",
    "    }\n",
    "    \n",
    "    features = features.rename(columns=feature_mapping)\n",
    "    \n",
    "    features['MAG_SQUARED'] = features['EQ_MAGNITUDE'] ** 2\n",
    "    features['IS_MAJOR_EQ'] = (features['EQ_MAGNITUDE'] >= 7.0).astype(int)\n",
    "    features['IS_SHALLOW'] = (features['EQ_DEPTH'] <= 70).astype(int)\n",
    "    \n",
    "    features['RISK_ZONE'] = features.apply(\n",
    "        lambda row: assign_risk_zone(row['LATITUDE'], row['LONGITUDE']), axis=1\n",
    "    )\n",
    "    \n",
    "    features['IS_OCEANIC'] = features.apply(\n",
    "        lambda row: is_oceanic(row['LATITUDE'], row['LONGITUDE']), axis=1\n",
    "    )\n",
    "    \n",
    "    features['MAG_CATEGORY'] = pd.cut(\n",
    "        features['EQ_MAGNITUDE'], \n",
    "        bins=[0, 4.5, 6.0, 7.0, 8.0, 10.0],\n",
    "        labels=['Weak', 'Moderate', 'Strong', 'Major', 'Great']\n",
    "    )\n",
    "    \n",
    "    features['DEPTH_CATEGORY'] = pd.cut(\n",
    "        features['EQ_DEPTH'],\n",
    "        bins=[0, 70, 300, 700, 800],\n",
    "        labels=['Shallow', 'Intermediate', 'Deep', 'Very_Deep']\n",
    "    )\n",
    "    \n",
    "    for feature_name, encoder in label_encoders.items():\n",
    "        if feature_name.replace('_ENCODED', '') in features.columns:\n",
    "            original_col = feature_name.replace('_ENCODED', '')\n",
    "            try:\n",
    "                feature_series = features[original_col].astype(str).fillna('Unknown')\n",
    "                known_mask = feature_series.isin(encoder.classes_)\n",
    "                if not known_mask.all():\n",
    "                    most_frequent_class = encoder.classes_[0] if len(encoder.classes_) > 0 else 'Unknown'\n",
    "                    feature_series.loc[~known_mask] = most_frequent_class\n",
    "                \n",
    "                features[feature_name] = encoder.transform(feature_series)\n",
    "            except:\n",
    "                if original_col in features.columns:\n",
    "                    unique_vals = features[original_col].unique()\n",
    "                    mapping = {val: i for i, val in enumerate(unique_vals)}\n",
    "                    features[feature_name] = features[original_col].map(mapping).fillna(0)\n",
    "                else:\n",
    "                    features[feature_name] = 0\n",
    "    \n",
    "    return features\n",
    "\n",
    "def assess_tsunami_risk(earthquake_df, model, scaler, feature_columns):\n",
    "    if earthquake_df.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    features = prepare_features_for_prediction(earthquake_df)\n",
    "    \n",
    "    available_columns = [col for col in feature_columns if col in features.columns]\n",
    "    missing_columns = [col for col in feature_columns if col not in features.columns]\n",
    "    \n",
    "    X_pred = features[available_columns].copy()\n",
    "    \n",
    "    for col in feature_columns:\n",
    "        if col not in X_pred.columns:\n",
    "            X_pred[col] = 0\n",
    "    \n",
    "    X_pred = X_pred.fillna(0)\n",
    "    \n",
    "    X_pred_scaled = scaler.transform(X_pred)\n",
    "    \n",
    "    predictions = model.predict(X_pred_scaled)\n",
    "    probabilities = model.predict_proba(X_pred_scaled)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    results = earthquake_df.copy()\n",
    "    results['tsunami_prediction'] = predictions\n",
    "    results['tsunami_probability'] = probabilities if probabilities is not None else predictions\n",
    "    results['risk_level'] = 'Low'\n",
    "    \n",
    "    if probabilities is not None:\n",
    "        results.loc[results['tsunami_probability'] >= 0.7, 'risk_level'] = 'High'\n",
    "        results.loc[(results['tsunami_probability'] >= 0.3) & (results['tsunami_probability'] < 0.7), 'risk_level'] = 'Medium'\n",
    "    else:\n",
    "        results.loc[results['tsunami_prediction'] == 1, 'risk_level'] = 'High'\n",
    "    \n",
    "    return results\n",
    "\n",
    "sample_earthquakes = pd.DataFrame({\n",
    "    'id': ['test1', 'test2', 'test3'],\n",
    "    'magnitude': [7.2, 6.8, 5.5],\n",
    "    'latitude': [38.0, -15.0, 35.0],\n",
    "    'longitude': [142.0, -175.0, 25.0],\n",
    "    'depth': [30.0, 15.0, 80.0],\n",
    "    'place': ['Japan', 'Pacific Ocean', 'Mediterranean'],\n",
    "    'time': [datetime.now()] * 3\n",
    "})\n",
    "\n",
    "threats = filter_tsunami_threats(sample_earthquakes)\n",
    "\n",
    "if len(threats) > 0:\n",
    "    risk_assessment = assess_tsunami_risk(threats, best_model, scaler, available_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79997d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tsunami_from_earthquake(magnitude, depth, latitude, longitude, model=None, scaler=None, feature_columns=None):\n",
    "    if model is None:\n",
    "        model = best_model\n",
    "    if scaler is None:\n",
    "        scaler = globals().get('scaler')\n",
    "    if feature_columns is None:\n",
    "        feature_columns = available_features\n",
    "    \n",
    "    earthquake_data = pd.DataFrame({\n",
    "        'magnitude': [magnitude],\n",
    "        'depth': [depth],\n",
    "        'latitude': [latitude],\n",
    "        'longitude': [longitude],\n",
    "        'id': ['prediction'],\n",
    "        'place': ['Unknown'],\n",
    "        'time': [datetime.now()]\n",
    "    })\n",
    "    \n",
    "    try:\n",
    "        risk_result = assess_tsunami_risk(earthquake_data, model, scaler, feature_columns)\n",
    "        \n",
    "        if len(risk_result) > 0:\n",
    "            prediction = {\n",
    "                'earthquake': {\n",
    "                    'magnitude': magnitude,\n",
    "                    'depth': depth,\n",
    "                    'latitude': latitude,\n",
    "                    'longitude': longitude\n",
    "                },\n",
    "                'tsunami_prediction': bool(risk_result.iloc[0]['tsunami_prediction']),\n",
    "                'tsunami_probability': float(risk_result.iloc[0]['tsunami_probability']),\n",
    "                'risk_level': risk_result.iloc[0]['risk_level'],\n",
    "                'is_oceanic': bool(is_oceanic(latitude, longitude)),\n",
    "                'risk_zone': assign_risk_zone(latitude, longitude),\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "        else:\n",
    "            prediction = {\n",
    "                'error': 'Could not process earthquake data',\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        prediction = {\n",
    "            'error': f'Prediction failed: {str(e)}',\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "test_cases = [\n",
    "    {\"name\": \"Major shallow oceanic earthquake\", \"mag\": 7.5, \"depth\": 25, \"lat\": 38.0, \"lon\": 142.0},\n",
    "    {\"name\": \"Moderate deep continental earthquake\", \"mag\": 6.2, \"depth\": 150, \"lat\": 40.0, \"lon\": -120.0},\n",
    "    {\"name\": \"Great shallow Pacific earthquake\", \"mag\": 8.2, \"depth\": 15, \"lat\": -15.0, \"lon\": -175.0},\n",
    "    {\"name\": \"Small shallow earthquake\", \"mag\": 5.0, \"depth\": 10, \"lat\": 35.0, \"lon\": 25.0}\n",
    "]\n",
    "\n",
    "for test in test_cases:\n",
    "    result = predict_tsunami_from_earthquake(test['mag'], test['depth'], test['lat'], test['lon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329bbd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "model_components = {\n",
    "    'model': best_model,\n",
    "    'scaler': scaler,\n",
    "    'feature_columns': available_features,\n",
    "    'label_encoders': label_encoders,\n",
    "    'model_name': best_model_name,\n",
    "    'performance_metrics': {\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'test_precision': test_precision,\n",
    "        'test_recall': test_recall,\n",
    "        'test_f1': test_f1\n",
    "    },\n",
    "    'training_date': datetime.now().isoformat(),\n",
    "    'training_data_shape': df_clean.shape\n",
    "}\n",
    "\n",
    "with open('models/tsunami_predictor_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model_components, f)\n",
    "\n",
    "def generate_tsunami_alert(prediction_result):\n",
    "    eq = prediction_result['earthquake']\n",
    "    \n",
    "    if prediction_result['tsunami_prediction']:\n",
    "        severity = prediction_result['risk_level'].upper()\n",
    "        print(f\"\\nüö® TSUNAMI ALERT - {severity} RISK üö®\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"üìç Location: {eq['latitude']:.2f}¬∞, {eq['longitude']:.2f}¬∞\")\n",
    "        print(f\"üìä Magnitude: {eq['magnitude']}\")\n",
    "        print(f\"üîΩ Depth: {eq['depth']} km\")\n",
    "        print(f\"üåä Probability: {prediction_result['tsunami_probability']:.1%}\")\n",
    "        print(f\"‚ö†Ô∏è Risk Level: {prediction_result['risk_level']}\")\n",
    "        print(f\"üåç Zone: {prediction_result['risk_zone']}\")\n",
    "        print(f\"üåä Oceanic: {'Yes' if prediction_result['is_oceanic'] else 'No'}\")\n",
    "        print(\"‚ö†Ô∏è TAKE IMMEDIATE PROTECTIVE ACTIONS ‚ö†Ô∏è\")\n",
    "        print(\"=\"*50)\n",
    "    else:\n",
    "        print(f\"\\n‚ÑπÔ∏è Low Risk Earthquake - Magnitude {eq['magnitude']}\")\n",
    "        print(f\"üåä Tsunami Probability: {prediction_result['tsunami_probability']:.1%}\")\n",
    "\n",
    "test_prediction = predict_tsunami_from_earthquake(7.8, 20, 38.0, 142.0)\n",
    "generate_tsunami_alert(test_prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
